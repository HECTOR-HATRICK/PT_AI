{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "042aba2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bab54940",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=os.listdir(\"splitcoords/train/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d67a0b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a single file as a numpy array\n",
    "def load_file(filepath):\n",
    "    dataframe = pd.read_csv(filepath, index_col=0)\n",
    "    return dataframe.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8a81cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a list of files into a 3D array of [samples, timesteps, features]\n",
    "def load_group(filenames, prefix=''):\n",
    "    loaded = list()\n",
    "    for name in filenames:\n",
    "        data = load_file(prefix + name)\n",
    "        loaded.append(data)\n",
    "    # stack group so that features are the 3rd dimension\n",
    "    loaded = np.dstack(loaded)\n",
    "    return loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1771b6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a dataset group, such as train or test\n",
    "def load_dataset_group(group, prefix=''):\n",
    "    filepath = prefix + group + '/data/'\n",
    "    # load all 9 files as a single array\n",
    "    filenames = cols\n",
    "    # load input data\n",
    "    X = load_group(filenames, filepath)\n",
    "    # load class output\n",
    "    y = load_file(prefix + group + '/y_'+group+'.csv')\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c7476bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(prefix=''):\n",
    "    # load all train\n",
    "    trainX, trainy = load_dataset_group('train')\n",
    "    print(trainX.shape, trainy.shape)\n",
    "    # load all test\n",
    "    testX, testy = load_dataset_group('val')\n",
    "    print(testX.shape, testy.shape)\n",
    "    # zero-offset class values\n",
    "    trainy = trainy - 1\n",
    "    testy = testy - 1\n",
    "    # one hot encode y\n",
    "    trainy = to_categorical(trainy)\n",
    "    testy = to_categorical(testy)\n",
    "    print(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
    "    return trainX, trainy, testX, testy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c943474c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"splitcoords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5d36cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "906af535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1137, 300, 99) (1137, 1)\n",
      "(298, 300, 99) (298, 1)\n",
      "(1137, 300, 99) (1137, 6) (298, 300, 99) (298, 6)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test=load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "91a62e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import ConvLSTM2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f799ed83",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "epochs=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d3c87d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and evaluate a model\n",
    "def evaluate_model(X_train, y_train, X_test, y_test):\n",
    "    verbose, epochs, batch_size = 0, 15, 64\n",
    "    n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], y_train.shape[1]\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, input_shape=(n_timesteps,n_features)))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # fit network\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    # evaluate model\n",
    "    _, accuracy = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=0)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6e188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat experiment\n",
    "scores1 = []\n",
    "for r in range(10):\n",
    "    score = evaluate_model(X_train, y_train, X_test, y_test)\n",
    "    score = score * 100.0\n",
    "    print('>#%d: %.3f' % (r+1, score))\n",
    "    scores1.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "081ffd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a9d20178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_results(scores):\n",
    "    print(scores)\n",
    "    m, s = mean(scores), std(scores)\n",
    "    print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07f0924",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_results(scores1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebca4569",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1D CNN added to model\n",
    "def evaluate_model_CNN(X_train, y_train, X_test, y_test):\n",
    "    # define model\n",
    "    verbose, epochs, batch_size = 0, 25, 64\n",
    "    n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], y_train.shape[1]\n",
    "    # reshape data into time steps of sub-sequences\n",
    "    n_steps, n_length = 10, 30\n",
    "    X_train = X_train.reshape((X_train.shape[0], n_steps, n_length, n_features))\n",
    "    X_test = X_test.reshape((X_test.shape[0], n_steps, n_length, n_features))\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu'), input_shape=(None,n_length,n_features)))\n",
    "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu')))\n",
    "    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # fit network\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    # evaluate model\n",
    "    _, accuracy = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=0)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59ff75ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">#1: 71.812\n",
      ">#2: 71.812\n",
      ">#3: 69.463\n",
      ">#4: 66.107\n",
      ">#5: 70.470\n",
      ">#6: 68.792\n",
      ">#7: 64.430\n",
      ">#8: 72.148\n",
      ">#9: 66.107\n",
      ">#10: 69.463\n"
     ]
    }
   ],
   "source": [
    "# repeat experiment with 1D CNN added\n",
    "scores2 = []\n",
    "for r in range(10):\n",
    "    score = evaluate_model_CNN(X_train, y_train, X_test, y_test)\n",
    "    score = score * 100.0\n",
    "    print('>#%d: %.3f' % (r+1, score))\n",
    "    scores2.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a6f57a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62.41610646247864, 68.45637559890747, 64.09395933151245, 59.06040072441101, 63.08724880218506, 53.35570573806763, 65.77181220054626, 67.1140968799591, 60.40268540382385, 62.41610646247864]\n",
      "Accuracy: 62.617% (+/-4.124)\n"
     ]
    }
   ],
   "source": [
    "summarize_results(scores2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0a3cb0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../splitcoords2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6a6b4620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">#1: 34.564\n",
      ">#2: 30.872\n",
      ">#3: 32.886\n",
      ">#4: 27.181\n",
      ">#5: 26.846\n",
      ">#6: 31.544\n",
      ">#7: 30.537\n",
      ">#8: 29.866\n",
      ">#9: 26.846\n",
      ">#10: 35.235\n"
     ]
    }
   ],
   "source": [
    "# repeat experiment on smoothed data\n",
    "scores3 = []\n",
    "for r in range(10):\n",
    "    score = evaluate_model(X_train, y_train, X_test, y_test)\n",
    "    score = score * 100.0\n",
    "    print('>#%d: %.3f' % (r+1, score))\n",
    "    scores3.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "448610de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34.563758969306946, 30.87248206138611, 32.88590610027313, 27.18120813369751, 26.8456369638443, 31.54362440109253, 30.536913871765137, 29.865771532058716, 26.8456369638443, 35.23489832878113]\n",
      "Accuracy: 30.638% (+/-2.895)\n"
     ]
    }
   ],
   "source": [
    "summarize_results(scores3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "acc394fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">#1: 65.101\n",
      ">#2: 66.443\n",
      ">#3: 67.785\n",
      ">#4: 71.812\n",
      ">#5: 69.128\n",
      ">#6: 66.107\n",
      ">#7: 71.141\n",
      ">#8: 67.114\n",
      ">#9: 62.416\n",
      ">#10: 67.114\n"
     ]
    }
   ],
   "source": [
    "# repeat experiment with 1D CNN added on smoothed data\n",
    "scores4 = []\n",
    "for r in range(10):\n",
    "    score = evaluate_model_CNN(X_train, y_train, X_test, y_test)\n",
    "    score = score * 100.0\n",
    "    print('>#%d: %.3f' % (r+1, score))\n",
    "    scores2.append(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
